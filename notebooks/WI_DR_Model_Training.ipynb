{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qrlw1P2-818w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Metrics for regression evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Preprocessing tools\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Main ML model\n",
        "import xgboost as xgb\n",
        "\n",
        "# Classical time-series models\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from prophet import Prophet\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfq-qpMbBaxG",
        "outputId": "14aeb703-5d80-4b43-c310-f544c4b15c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (37345, 41)\n",
            "Columns: Index(['date', 'zone_id', 'zone_name', 'population', 'waste_baseline_tons',\n",
            "       'dow', 'month', 'is_umuganda', 'event_count_cal', 'event_intensity_cal',\n",
            "       'rain_mm', 'temp_c', 'humidity', 'ndvi', 'season_month', 'season_week',\n",
            "       'event_factor', 'weather_factor', 'ndvi_factor', 'waste_tons_day',\n",
            "       'year', 'day', 'is_weekend', 'weekofyear', 'month_sin', 'month_cos',\n",
            "       'dow_sin', 'dow_cos', 'waste_tons_day_lag1', 'waste_tons_day_lag7',\n",
            "       'waste_tons_day_lag14', 'waste_tons_day_lag28',\n",
            "       'waste_tons_day_roll7_mean', 'waste_tons_day_roll14_mean',\n",
            "       'waste_tons_day_roll28_mean', 'ndvi_lag1', 'ndvi_lag7', 'ndvi_lag14',\n",
            "       'ndvi_roll7_mean', 'ndvi_roll14_mean', 'ndvi_roll28_mean'],\n",
            "      dtype='object')\n",
            "Train shape: (24570, 41)\n",
            "Val shape: (6335, 41)\n",
            "Test shape: (6440, 41)\n",
            "Train max date: 2022-12-31 00:00:00\n",
            "Test min date: 2023-07-01 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# Load your dataset\n",
        "df = pd.read_csv(\"widr_zone_day_model_ready_2021_2023.csv\")\n",
        "\n",
        "# Convert date column to datetime and sort by zone and time\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.sort_values([\"zone_id\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "# Ensure target exists\n",
        "target = \"waste_tons_day\"\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns)\n",
        "# Create time-based split\n",
        "\n",
        "# Choose split dates\n",
        "train_end = \"2022-12-31\"\n",
        "val_end   = \"2023-06-30\"\n",
        "\n",
        "train = df[df[\"date\"] <= train_end].copy()\n",
        "val   = df[(df[\"date\"] > train_end) & (df[\"date\"] <= val_end)].copy()\n",
        "test  = df[df[\"date\"] > val_end].copy()\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Val shape:\", val.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "# Safety check\n",
        "print(\"Train max date:\", train[\"date\"].max())\n",
        "print(\"Test min date:\", test[\"date\"].min())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def safe_metrics(y_true, y_pred):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    # Only evaluate where both true and predicted values are finite\n",
        "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "    rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "    return mae, rmse\n",
        "\n",
        "\n",
        "def eval_naive_lag1(test_df, target_col, lag1_col=\"waste_tons_day_lag1\"):\n",
        "    \"\"\"\n",
        "    Naive baseline model:\n",
        "    Predict today's waste using yesterday's waste (Lag-1).\n",
        "    This is a strong baseline for time-series forecasting.\n",
        "    \"\"\"\n",
        "    if lag1_col not in test_df.columns:\n",
        "        raise ValueError(f\"Missing '{lag1_col}'. Create lag features first.\")\n",
        "\n",
        "    return test_df[lag1_col].values\n",
        "\n",
        "\n",
        "def train_xgb_forecaster(train_df, test_df, target_col):\n",
        "    \"\"\"\n",
        "    Train an XGBoost regression model for waste forecasting.\n",
        "\n",
        "    Model pipeline:\n",
        "    - One-hot encode zone_id (categorical spatial identifier)\n",
        "    - Pass all numeric engineered features unchanged\n",
        "    - Train gradient-boosted regression trees\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove non-feature columns\n",
        "    drop_cols = [\"date\", \"zone_name\", target_col]\n",
        "    drop_cols = [c for c in drop_cols if c in train_df.columns]\n",
        "\n",
        "    features = [c for c in train_df.columns if c not in drop_cols]\n",
        "\n",
        "    # Separate categorical and numeric inputs\n",
        "    categorical_cols = [\"zone_id\"]\n",
        "    numeric_cols = [c for c in features if c not in categorical_cols]\n",
        "\n",
        "    # Preprocessing: encode spatial ID, keep numeric signals\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "    ])\n",
        "\n",
        "    # XGBoost regression model (main forecasting engine)\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=1200,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Combine preprocessing + model into a single pipeline\n",
        "    model = Pipeline([\n",
        "        (\"prep\", preprocessor),\n",
        "        (\"model\", xgb_model)\n",
        "    ])\n",
        "\n",
        "    # Train model\n",
        "    X_train, y_train = train_df[features], train_df[target_col]\n",
        "    X_test = test_df[features]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Generate predictions for evaluation period\n",
        "    pred = model.predict(X_test)\n",
        "\n",
        "    return pred, model\n"
      ],
      "metadata": {
        "id": "MQNfBu5KSbUm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical baselines: ARIMA and Prophet\n",
        "\n",
        "def arima_forecast_per_zone(train_df, test_df, target_col,\n",
        "                            zone_col=\"zone_id\", date_col=\"date\",\n",
        "                            order=(2, 1, 2)):\n",
        "    \"\"\"\n",
        "    ARIMA forecasting baseline applied independently per zone.\n",
        "    Captures temporal autocorrelation but ignores exogenous/context features.\n",
        "    \"\"\"\n",
        "    out_rows = []\n",
        "    zones = sorted(test_df[zone_col].unique())\n",
        "\n",
        "    for z in zones:\n",
        "        tr = train_df[train_df[zone_col] == z].sort_values(date_col)\n",
        "        te = test_df[test_df[zone_col] == z].sort_values(date_col)\n",
        "\n",
        "        # Skip zones without enough history for ARIMA fitting\n",
        "        if len(tr) < 60 or len(te) == 0:\n",
        "            continue\n",
        "\n",
        "        # Fit ARIMA on historical target signal\n",
        "        fit = ARIMA(tr[target_col].values, order=order).fit()\n",
        "        fc = fit.forecast(steps=len(te))\n",
        "\n",
        "        out_rows.append(pd.DataFrame({\n",
        "            zone_col: z,\n",
        "            date_col: te[date_col].values,\n",
        "            \"pred_arima\": fc\n",
        "        }))\n",
        "\n",
        "    if not out_rows:\n",
        "        return pd.DataFrame(columns=[zone_col, date_col, \"pred_arima\"])\n",
        "\n",
        "    return pd.concat(out_rows, ignore_index=True)\n",
        "\n",
        "\n",
        "def prophet_forecast_per_zone(train_df, test_df, target_col,\n",
        "                              zone_col=\"zone_id\", date_col=\"date\"):\n",
        "    \"\"\"\n",
        "    Prophet forecasting baseline applied independently per zone.\n",
        "    Captures seasonality (weekly + yearly) but ignores engineered ML features.\n",
        "    \"\"\"\n",
        "    out_rows = []\n",
        "    zones = sorted(test_df[zone_col].unique())\n",
        "\n",
        "    for z in zones:\n",
        "        tr = train_df[train_df[zone_col] == z].sort_values(date_col)\n",
        "        te = test_df[test_df[zone_col] == z].sort_values(date_col)\n",
        "\n",
        "        if len(tr) < 60 or len(te) == 0:\n",
        "            continue\n",
        "\n",
        "        # Prepare data for Prophet format\n",
        "        prophet_train = tr[[date_col, target_col]].rename(\n",
        "            columns={date_col: \"ds\", target_col: \"y\"}\n",
        "        )\n",
        "\n",
        "        # Prophet model with seasonal components\n",
        "        m = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False\n",
        "        )\n",
        "        m.fit(prophet_train)\n",
        "\n",
        "        future = pd.DataFrame({\"ds\": te[date_col].values})\n",
        "        fc = m.predict(future)\n",
        "\n",
        "        out_rows.append(pd.DataFrame({\n",
        "            zone_col: z,\n",
        "            date_col: te[date_col].values,\n",
        "            \"pred_prophet\": fc[\"yhat\"].values\n",
        "        }))\n",
        "\n",
        "    if not out_rows:\n",
        "        return pd.DataFrame(columns=[zone_col, date_col, \"pred_prophet\"])\n",
        "\n",
        "    return pd.concat(out_rows, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "xcq8VQIIeXwE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions from all models\n",
        "pred_lag1 = eval_naive_lag1(test, target)\n",
        "pred_xgb, xgb_pipe = train_xgb_forecaster(train, test, target)\n",
        "\n",
        "# Generate classical baseline forecasts\n",
        "arima_out = arima_forecast_per_zone(train, test, target)\n",
        "prophet_out = prophet_forecast_per_zone(train, test, target)\n",
        "\n",
        "# Build evaluation dataframe aligned by date and zone\n",
        "eval_df = test[[\"date\", \"zone_id\", target]].copy()\n",
        "eval_df[\"pred_lag1\"] = pred_lag1\n",
        "eval_df[\"pred_xgb\"] = pred_xgb\n",
        "\n",
        "# Merge ARIMA and Prophet predictions into same table\n",
        "eval_df = eval_df.merge(arima_out, on=[\"zone_id\", \"date\"], how=\"left\")\n",
        "eval_df = eval_df.merge(prophet_out, on=[\"zone_id\", \"date\"], how=\"left\")\n",
        "\n",
        "# Compute performance metrics for each model\n",
        "mae_lag1, rmse_lag1 = safe_metrics(eval_df[target], eval_df[\"pred_lag1\"])\n",
        "mae_arima, rmse_arima = safe_metrics(eval_df[target], eval_df[\"pred_arima\"])\n",
        "mae_prophet, rmse_prophet = safe_metrics(eval_df[target], eval_df[\"pred_prophet\"])\n",
        "mae_xgb, rmse_xgb = safe_metrics(eval_df[target], eval_df[\"pred_xgb\"])\n",
        "\n",
        "# Create a single comparison table\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Naive Lag-1\", \"ARIMA\", \"Prophet\", \"XGBoost\"],\n",
        "    \"MAE\": [mae_lag1, mae_arima, mae_prophet, mae_xgb],\n",
        "    \"RMSE\": [rmse_lag1, rmse_arima, rmse_prophet, rmse_xgb]\n",
        "}).sort_values(\"MAE\")\n",
        "\n",
        "print(results)\n",
        "\n",
        "# Attach zone_name if available (for readable output)\n",
        "if \"zone_name\" in df.columns:\n",
        "    zone_lookup = df[[\"zone_id\", \"zone_name\"]].drop_duplicates()\n",
        "    forecast_out = eval_df[[\"date\", \"zone_id\"]].merge(zone_lookup, on=\"zone_id\", how=\"left\")\n",
        "else:\n",
        "    forecast_out = eval_df[[\"date\", \"zone_id\"]].copy()\n",
        "\n",
        "# Export XGBoost predictions to feed VRP routing module\n",
        "forecast_out[\"pred_waste_tons_day\"] = eval_df[\"pred_xgb\"].values\n",
        "forecast_out.to_csv(\"forecast_zone_day.csv\", index=False)\n",
        "print(\"Saved forecast file: forecast_zone_day.csv\")\n",
        "\n",
        "# Compute relative error (forecast accuracy vs average waste level)\n",
        "mean_waste = df[target].mean()\n",
        "mae_pct = (mae_xgb / mean_waste) * 100 if mean_waste else np.nan\n",
        "\n",
        "print(\"Mean waste:\", mean_waste)\n",
        "print(\"Relative MAE (%):\", mae_pct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr_CyVpHeemy",
        "outputId": "82bfcd4c-3533-435a-dcb5-6098d92dfeb8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Model       MAE      RMSE\n",
            "3      XGBoost  0.801528  1.171977\n",
            "2      Prophet  1.190590  1.813681\n",
            "0  Naive Lag-1  1.570418  2.357977\n",
            "1        ARIMA  2.037998  2.773244\n",
            "Saved forecast file: forecast_zone_day.csv\n",
            "Mean waste: 29.752307339324133\n",
            "Relative MAE (%): 2.694001522018811\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}