{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_kk1SFaL-55Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Historical Waste Dataset Generation**\n",
        "\n",
        "The historical dataset was generated using report-based baseline indicators derived from Rwanda’s waste management literature. Population data for all Kigali sectors were obtained from the 2022 Rwanda Population and Housing Census published by the National Institute of Statistics of Rwanda (NISR).\n",
        "\n",
        "Daily baseline waste generation was computed using the Kigali per-capita waste generation rate reported by the Global Green Growth Institute (GGGI, 2023), estimated at 0.56 kg per capita per day. The baseline daily waste for each zone was calculated using:\n",
        "\n",
        "\n",
        "\\[\n",
        "\\text{Waste (tons/day)} = \\frac{\\text{Population} \\times 0.56}{1000}\n",
        "\\]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A continuous daily time series was generated for the period 2021–2023. Contextual temporal features were included to support forecasting, including day-of-week, month, and an Umuganda indicator representing the last Saturday of each month, which reflects localized variations in waste generation patterns.\n",
        "\n",
        "The resulting dataset forms the baseline historical input for the forecasting component of the WI-DR system, providing structured daily waste estimates for each sector prior to preprocessing, feature engineering, and model training.\n",
        "\n",
        "\n",
        "The resulting dataset forms the baseline historical input for the forecasting component of the WI-DR system, providing structured daily waste estimates for each pilot zone prior to preprocessing, feature engineering, and model training.\n"
      ],
      "metadata": {
        "id": "RNgJH2hTErFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMD2dVgf2XrQ"
      },
      "outputs": [],
      "source": [
        "#Import neccessary libaries\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your population CSV\n",
        "POP_PATH = \"/content/drive/MyDrive/WI-DR/kigali_sectors_population.csv\"\n",
        "\n",
        "# Dataset time range\n",
        "START_DATE = \"2021-01-01\"\n",
        "END_DATE   = \"2023-12-31\"\n",
        "\n",
        "# Kigali waste generation rate (kg per capita per day)\n",
        "KG_PER_CAPITA_PER_DAY = 0.56\n",
        "\n",
        "\n",
        "# LOAD POPULATION DATA\n",
        "pop_df = pd.read_csv(POP_PATH)\n",
        "pop_df.columns = [c.strip().lower() for c in pop_df.columns]\n",
        "\n",
        "print(\"Detected columns:\", pop_df.columns)\n",
        "\n",
        "# Use correct columns and rename\n",
        "zones = pop_df[[\"sector_id\", \"sector_name\", \"population_2022\"]].copy()\n",
        "zones.rename(columns={\n",
        "    \"sector_id\": \"zone_id\",\n",
        "    \"sector_name\": \"zone_name\",\n",
        "    \"population_2022\": \"population\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Clean population\n",
        "zones[\"population\"] = pd.to_numeric(zones[\"population\"], errors=\"coerce\")\n",
        "zones = zones.dropna(subset=[\"population\"])\n",
        "zones[\"population\"] = zones[\"population\"].astype(int)\n",
        "\n",
        "# Compute baseline daily waste (tons)\n",
        "zones[\"waste_baseline_tons\"] = (zones[\"population\"] * KG_PER_CAPITA_PER_DAY) / 1000.0\n",
        "\n",
        "\n",
        "# GENERATE DATE RANGE\n",
        "dates = pd.date_range(START_DATE, END_DATE, freq=\"D\")\n",
        "\n",
        "\n",
        "# UMUGANDA DETECTION\n",
        "def is_last_saturday(d):\n",
        "    if d.weekday() != 5:\n",
        "        return 0\n",
        "    return 1 if (d + timedelta(days=7)).month != d.month else 0\n",
        "\n",
        "\n",
        "# BUILD DATASET\n",
        "rows = []\n",
        "\n",
        "for d in dates:\n",
        "    dow = d.weekday()\n",
        "    month = d.month\n",
        "    umug = is_last_saturday(d)\n",
        "\n",
        "    for _, r in zones.iterrows():\n",
        "        rows.append({\n",
        "            \"date\": d.strftime(\"%Y-%m-%d\"),\n",
        "            \"zone_id\": r[\"zone_id\"],        # SEC_01 etc\n",
        "            \"zone_name\": r[\"zone_name\"],    # REAL SECTOR NAME\n",
        "            \"population\": int(r[\"population\"]),\n",
        "            \"waste_baseline_tons\": round(float(r[\"waste_baseline_tons\"]), 6),\n",
        "            \"dow\": dow,\n",
        "            \"month\": month,\n",
        "            \"is_umuganda\": umug\n",
        "        })\n",
        "\n",
        "hist = pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# SAVE DATASET\n",
        "OUTPUT_FILE = \"historical_waste_kigali_sectors.csv\"\n",
        "hist.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "\n",
        "# OUTPUT CHECK\n",
        "print(\"Zones:\", zones.shape[0])\n",
        "print(\"Dataset shape:\", hist.shape)\n",
        "print(hist.head())\n",
        "print(\"\\nSaved to:\", OUTPUT_FILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5fi6YmhEMJd",
        "outputId": "79ed8ead-6d8e-4312-8fa5-00cc154fd645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Detected columns: Index(['sector_id', 'sector_name', 'population_2022', 'district',\n",
            "       'pop_share_2022', 'sector_msw_tpd_baseline'],\n",
            "      dtype='object')\n",
            "Zones: 35\n",
            "Dataset shape: (38325, 8)\n",
            "         date zone_id   zone_name  population  waste_baseline_tons  dow  \\\n",
            "0  2021-01-01  SEC_01      Gitega       26668             14.93408    4   \n",
            "1  2021-01-01  SEC_02    Kanyinya       31026             17.37456    4   \n",
            "2  2021-01-01  SEC_03      Kigali       61499             34.43944    4   \n",
            "3  2021-01-01  SEC_04  Kimisagara       56534             31.65904    4   \n",
            "4  2021-01-01  SEC_05  Mageregere       59747             33.45832    4   \n",
            "\n",
            "   month  is_umuganda  \n",
            "0      1            0  \n",
            "1      1            0  \n",
            "2      1            0  \n",
            "3      1            0  \n",
            "4      1            0  \n",
            "\n",
            "Saved to: historical_waste_kigali_sectors.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contextual Data Integration — Weather Dataset**\n",
        "\n",
        "To improve waste-demand prediction accuracy, daily weather data was integrated as an exogenous input into the WI-DR forecasting pipeline. Weather conditions influence waste generation through moisture effects, decomposition rates, and operational disruptions. The weather dataset was generated using the Open-Meteo Historical Archive API for Kigali, Rwanda.\n",
        "\n",
        "\n",
        "\n",
        "### Location Parameters\n",
        "Weather data was retrieved using the geographic coordinates of Kigali:\n",
        "\n",
        "- Latitude: -1.9441  \n",
        "- Longitude: 30.0619  \n",
        "\n",
        "These coordinates represent central Kigali and provide city-level daily weather signals suitable for zone-day modeling.\n",
        "\n",
        "\n",
        "\n",
        "### Data Source\n",
        "Open-Meteo Historical Weather Archive\n",
        "\n",
        "Endpoint used:\n",
        "https://archive-api.open-meteo.com/v1/archive\n",
        "\n",
        "---\n",
        "\n",
        "### Weather Variables Collected\n",
        "\n",
        "| Variable (API) | Dataset Column | Description |\n",
        "|---------------|---------------|-------------|\n",
        "| precipitation_sum | rain_mm | Total daily rainfall (mm) |\n",
        "| temperature_2m_mean | temp_c | Mean daily temperature (°C) |\n",
        "| relative_humidity_2m_mean | humidity | Mean daily relative humidity (%) |"
      ],
      "metadata": {
        "id": "n7Q-HlGU3y-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define location (Kigali coordinates)\n",
        "LAT = -1.9441   # Latitude of Kigali, Rwanda\n",
        "LON = 30.0619   # Longitude of Kigali, Rwanda\n",
        "\n",
        "# Define time range for data extraction\n",
        "START = \"2021-01-01\"   # Start date of historical weather data\n",
        "END = \"2023-12-31\"     # End date of historical weather data\n",
        "\n",
        "# Build Open-Meteo API request URL\n",
        "# This request asks for daily weather data including:\n",
        "# Mean temperature, total daily precipitation, and mean relative humidity.\n",
        "# Timezone is set to auto so dates align with Kigali local time.\n",
        "url = (\n",
        "    \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "    f\"?latitude={LAT}\"\n",
        "    f\"&longitude={LON}\"\n",
        "    f\"&start_date={START}\"\n",
        "    f\"&end_date={END}\"\n",
        "    \"&daily=temperature_2m_mean,precipitation_sum,relative_humidity_2m_mean\"\n",
        "    \"&timezone=auto\"\n",
        ")\n",
        "\n",
        "# Send request to Open-Meteo API\n",
        "response = requests.get(url)     # Fetch weather data from API\n",
        "data = response.json()           # Convert JSON response into Python dictionary\n",
        "\n",
        "# Convert JSON to structured DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"date\": data[\"daily\"][\"time\"],                          # Observation date\n",
        "    \"rain_mm\": data[\"daily\"][\"precipitation_sum\"],          # Daily rainfall (mm)\n",
        "    \"temp_c\": data[\"daily\"][\"temperature_2m_mean\"],         # Mean temperature (°C)\n",
        "    \"humidity\": data[\"daily\"][\"relative_humidity_2m_mean\"]  # Mean relative humidity (%)\n",
        "})\n",
        "\n",
        "# Export dataset to CSV for integration into WI-DR pipeline\n",
        "df.to_csv(\"weather_daily_kigali.csv\", index=False)\n",
        "\n",
        "# Confirmation message\n",
        "print(\"Weather dataset with humidity generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qG9ccJa3Srd",
        "outputId": "fbb8f4b7-10d5-41d1-b2bf-62fb264d8790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather dataset with humidity generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contextual Data Integration — Calendar & Holiday Events Dataset**\n",
        "\n",
        "To improve waste-demand prediction accuracy, calendar-based event signals were integrated as an exogenous input into the WI-DR forecasting pipeline. Public holidays and community activity days influence waste generation through changes in household behavior, commercial activity, and temporary waste spikes. The holiday dataset was generated by extracting official Rwanda public holidays for the period 2021–2023.\n",
        "\n",
        "\n",
        "\n",
        "**Coverage Period**\n",
        "\n",
        "Calendar events were collected for the full modeling horizon: January 1, 2021 → December 31, 2023. Daily resolution\n",
        "\n",
        "\n",
        "**Data Source**\n",
        "\n",
        "Rwanda Public Holiday Calendar — TimeandDate\n",
        "\n",
        "Pages used:\n",
        "\n",
        "https://www.timeanddate.com/holidays/rwanda/2021\n",
        "\n",
        "https://www.timeanddate.com/holidays/rwanda/2022\n",
        "\n",
        "https://www.timeanddate.com/holidays/rwanda/2023\n",
        "\n",
        "Only official public holidays were retained; observances and non-official events were excluded.\n",
        "\n",
        "Event Variables Collected:\n",
        "Raw Column\tDataset Column\tDescription\n",
        "date\tdate\tCalendar date (YYYY-MM-DD)\n",
        "holiday name\tname\tOfficial holiday name\n",
        "source\tsource\tData origin identifier\n",
        "Derived Calendar Features (Used in WI-DR)\n",
        "\n",
        "After integration into the zone-day dataset, the following modeling features were generated:\n",
        "\n",
        "\n",
        "Feature\tDescription\n",
        "is_holiday\tBinary indicator (1 if public holiday, else 0),\n",
        "event_count_cal\tNumber of calendar events on that day, event_intensity_cal\tWeighted influence of calendar events on waste demand"
      ],
      "metadata": {
        "id": "zujonmusIQ9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import re\n",
        "import pandas as pd\n",
        "\n",
        "# URLs for Rwanda holidays by year (Timeanddate)\n",
        "YEAR_URLS = {\n",
        "    2021: \"https://www.timeanddate.com/holidays/rwanda/2021?hol=1\",\n",
        "    2022: \"https://www.timeanddate.com/holidays/rwanda/2022?hol=1\",\n",
        "    2023: \"https://www.timeanddate.com/holidays/rwanda/2023?hol=1\",\n",
        "}\n",
        "\n",
        "def normalize_cols(cols):\n",
        "    \"\"\"\n",
        "    Flatten and normalize column names to lowercase strings.\n",
        "    Handles MultiIndex headers and cleans extra spaces/symbols.\n",
        "    \"\"\"\n",
        "    # Flatten MultiIndex headers\n",
        "    if isinstance(cols, pd.MultiIndex):\n",
        "        flat = []\n",
        "        for tup in cols.values:\n",
        "            joined = \" \".join([str(x) for x in tup if x and str(x) != \"nan\"]).strip()\n",
        "            flat.append(joined)\n",
        "    else:\n",
        "        flat = [str(c).strip() for c in cols]\n",
        "\n",
        "    # Lowercase + clean non-alphanumeric (keeps spaces)\n",
        "    clean = []\n",
        "    for c in flat:\n",
        "        c = c.lower()\n",
        "        c = re.sub(r\"[^a-z0-9 ]+\", \"\", c)  # remove punctuation\n",
        "        c = re.sub(r\"\\s+\", \" \", c).strip() # collapse spaces\n",
        "        clean.append(c)\n",
        "\n",
        "    return clean\n",
        "\n",
        "def has_date_and_name_like(cols_norm):\n",
        "    \"\"\"\n",
        "    Returns True if columns contain something like:\n",
        "    - date (e.g. 'date', 'date day', etc.)\n",
        "    - and name/holiday (e.g. 'name', 'holiday', 'holiday name')\n",
        "    \"\"\"\n",
        "    has_date = any(\"date\" in c for c in cols_norm)\n",
        "    has_name = any((\"name\" in c) or (\"holiday\" in c) for c in cols_norm)\n",
        "    return has_date and has_name\n",
        "\n",
        "all_rows = []\n",
        "\n",
        "for year, url in YEAR_URLS.items():\n",
        "\n",
        "    # Pull all tables from the page\n",
        "    tables = pd.read_html(url)\n",
        "\n",
        "    chosen = None\n",
        "    chosen_cols = None\n",
        "\n",
        "    # Try to find the holiday table using \"contains\" matching\n",
        "    for t in tables:\n",
        "        cols_norm = normalize_cols(t.columns)\n",
        "\n",
        "        if has_date_and_name_like(cols_norm):\n",
        "            chosen = t.copy()\n",
        "            chosen_cols = cols_norm\n",
        "            break\n",
        "\n",
        "    # If still not found, print table column options for debugging\n",
        "    if chosen is None:\n",
        "        print(f\"\\n[DEBUG] Could not auto-detect holiday table for {year}.\")\n",
        "        print(\"These are the tables I found and their normalized columns:\\n\")\n",
        "        for i, t in enumerate(tables):\n",
        "            print(i, normalize_cols(t.columns))\n",
        "        raise RuntimeError(f\"Could not find holiday table for year {year} at {url}\")\n",
        "\n",
        "    # Apply normalized columns\n",
        "    chosen.columns = chosen_cols\n",
        "\n",
        "    # Identify which column is the \"date-like\" column\n",
        "    date_col = next(c for c in chosen.columns if \"date\" in c)\n",
        "\n",
        "    # Identify which column is the \"name/holiday-like\" column\n",
        "    name_col = next(c for c in chosen.columns if (\"name\" in c) or (\"holiday\" in c))\n",
        "\n",
        "    # If a \"type\" column exists, keep only \"Public Holiday\"\n",
        "\n",
        "    if \"type\" in chosen.columns:\n",
        "        chosen = chosen[\n",
        "            chosen[\"type\"].astype(str).str.contains(\"Public Holiday\", case=False, na=False)\n",
        "        ]\n",
        "\n",
        "    # Parse dates\n",
        "    def parse_date(x):\n",
        "        x = str(x).strip()\n",
        "        if str(year) in x:\n",
        "            return pd.to_datetime(x, errors=\"coerce\")\n",
        "        return pd.to_datetime(f\"{x} {year}\", errors=\"coerce\")\n",
        "\n",
        "    chosen[\"date\"] = chosen[date_col].apply(parse_date)\n",
        "    chosen = chosen.dropna(subset=[\"date\"])\n",
        "\n",
        "    # Build output table\n",
        "    out = pd.DataFrame({\n",
        "        \"date\": chosen[\"date\"],\n",
        "        \"name\": chosen[name_col].astype(str).str.strip(),\n",
        "        \"source\": \"timeanddate\",\n",
        "    })\n",
        "\n",
        "    all_rows.append(out)\n",
        "\n",
        "# Combine all years + export\n",
        "holidays = (\n",
        "    pd.concat(all_rows, ignore_index=True)\n",
        "      .drop_duplicates(subset=[\"date\", \"name\"])\n",
        "      .sort_values(\"date\")\n",
        ")\n",
        "\n",
        "holidays[\"date\"] = holidays[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "holidays.to_csv(\"rwanda_holidays.csv\", index=False)\n",
        "\n",
        "print(\"\\n Saved: rwanda_holidays.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6i9Z8z7J3HZ",
        "outputId": "665ae79e-fcf1-43b5-d253-4c7398b93a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Saved: rwanda_holidays.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "baseline = pd.read_csv(\"historical_waste_kigali_sectors.csv\")\n",
        "baseline[\"date\"] = pd.to_datetime(baseline[\"date\"])\n",
        "\n",
        "hol = pd.read_csv(\"rwanda_holidays.csv\")\n",
        "hol[\"date\"] = pd.to_datetime(hol[\"date\"])\n",
        "holiday_set = set(hol[\"date\"].dt.date)\n",
        "\n",
        "baseline[\"is_holiday\"] = baseline[\"date\"].dt.date.isin(holiday_set).astype(int)\n",
        "\n",
        "# Umuganda = last Saturday of each month\n",
        "def is_last_saturday(dt):\n",
        "    return int(dt.weekday() == 5 and (dt + pd.Timedelta(days=7)).month != dt.month)\n",
        "\n",
        "baseline[\"is_umuganda\"] = baseline[\"date\"].apply(is_last_saturday)\n",
        "\n",
        "# Calendar-based event_count and intensity (deterministic, real)\n",
        "baseline[\"event_count_cal\"] = baseline[\"is_holiday\"] + baseline[\"is_umuganda\"]\n",
        "\n",
        "# Intensity weights\n",
        "baseline[\"event_intensity_cal\"] = (\n",
        "    baseline[\"is_umuganda\"] * 40 +\n",
        "    baseline[\"is_holiday\"] * 70\n",
        ").clip(0, 100).astype(float)\n",
        "\n",
        "out = baseline[[\"date\", \"zone_id\", \"event_count_cal\", \"event_intensity_cal\"]].copy()\n",
        "out[\"date\"] = out[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "out.to_csv(\"events_calendar_zone_daily_2021_2023.csv\", index=False)\n",
        "\n",
        "print(\"Saved: events_calendar_zone_daily_2021_2023.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX2sHGmyPwFQ",
        "outputId": "f1cc0566-e624-49b7-9561-0a66d5646ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: events_calendar_zone_daily_2021_2023.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contextual Data Integration — Satellite Vegetation (NDVI) Dataset**\n",
        "\n",
        "To enhance environmental awareness within the WI-DR forecasting pipeline, satellite-derived vegetation signals were integrated as an exogenous input. Vegetation density and seasonal biomass cycles influence organic waste generation, decomposition rates, and environmental conditions across urban zones. The  NDVI (Normalized Difference Vegetation Index) dataset was generated using Sentinel-2 satellite imagery processed through Google Earth Engine for Kigali sectors over the period 2021–2023.\n",
        "\n",
        "**Coverage Period**\n",
        "\n",
        "Satellite vegetation data was collected for the full modeling horizon:\n",
        "\n",
        "January 1, 2021 → December 31, 2023\n",
        "Daily resolution (derived from monthly satellite observations)\n",
        "\n",
        "**Data Source**\n",
        "\n",
        "Sentinel-2 Surface Reflectance (SR) Harmonized — Google Earth Engine\n",
        "\n",
        "Vegetation index computed using:\n",
        "\n",
        "NDVI = (B8 − B4) / (B8 + B4)\n",
        "\n",
        "**Where:**\n",
        "\n",
        "B8 = Near-Infrared band\n",
        "\n",
        "B4 = Red band\n",
        "\n",
        "**Spatial Preparation**\n",
        "\n",
        "Kigali administrative sector boundaries were processed using GeoPandas:\n",
        "\n",
        "- Sector column automatically detected and standardized  \n",
        "- Dataset filtered to Kigali districts *(Gasabo, Kicukiro, Nyarugenge...)*  \n",
        "- Cleaned and normalized `zone_id` created  \n",
        "- Polygons dissolved to produce **35 Kigali sectors**  \n",
        "- Reprojected to **EPSG:4326** and exported as **GeoJSON** and **Shapefile** for Google Earth Engine use  \n",
        "\n",
        "\n",
        "**Satellite Variables Collected**\n",
        "\n",
        "| Raw Variable | Dataset Column | Description |\n",
        "|-------------|---------------|-------------|\n",
        "| NDVI | ndvi | Vegetation index representing vegetation density and biomass |\n",
        "| date | date | Observation date |\n",
        "| zone_id | zone_id | Kigali sector identifier |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Temporal Processing**\n",
        "\n",
        "Satellite NDVI was originally available at monthly resolution and processed as follows:\n",
        "\n",
        "\n",
        "*   Monthly NDVI exported per sector for 2021–2023\n",
        "*   All yearly datasets merged into one continuous monthly dataset\n",
        "*   Duplicate records removed and sorted chronologically\n",
        "*   Monthly NDVI expanded to daily resolution\n",
        "*   Forward-fill applied per sector to maintain temporal continuity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "grlfVYkULzrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load file\n",
        "INPUT_GEOJSON = \"kigali_sectors.geojson.json\"\n",
        "gdf = gpd.read_file(INPUT_GEOJSON)\n",
        "\n",
        "print(\"Columns:\", list(gdf.columns))\n",
        "print(\"CRS:\", gdf.crs)\n",
        "print(\"Rows:\", len(gdf))\n",
        "\n",
        "# Helpers\n",
        "def clean_text(s):\n",
        "    s = s.astype(str)\n",
        "    s = s.str.strip()\n",
        "    s = s.str.replace(r\"\\s+\", \" \", regex=True)  # collapse whitespace\n",
        "    s = s.str.replace(\"\\u00a0\", \" \", regex=False)  # non-breaking spaces\n",
        "    s = s.str.lower()\n",
        "    return s\n",
        "\n",
        "def pick_best_zone_col(df, candidates):\n",
        "    best = None\n",
        "    best_uniques = None\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            n = clean_text(df[c]).nunique()\n",
        "            print(f\"[candidate] {c}: unique={n}\")\n",
        "            # Prefer something close to 35 (or at least much smaller than 381)\n",
        "            if best is None or abs(n - 35) < abs(best_uniques - 35):\n",
        "                best, best_uniques = c, n\n",
        "    return best, best_uniques\n",
        "\n",
        "# Choose a sector column\n",
        "CANDIDATES = [\n",
        "    \"zone_id\", \"sector\", \"Sector\", \"sector_name\", \"SECTOR\",\n",
        "    \"NAME_3\", \"NAME\", \"name\", \"admin3Name\", \"ADM3_EN\", \"ADM3_NAME\",\n",
        "    \"ADM3\", \"adm3\", \"SNAME\", \"S_NAME\", \"sectorName\"\n",
        "]\n",
        "\n",
        "zone_col, zone_uniques = pick_best_zone_col(gdf, CANDIDATES)\n",
        "if zone_col is None:\n",
        "    raise ValueError(\"Could not find a usable sector column. Paste the printed Columns list.\")\n",
        "\n",
        "print(\"Chosen zone_col:\", zone_col, \"| uniques:\", zone_uniques)\n",
        "\n",
        "# Filter to Kigali using admin fields\n",
        "ADMIN_CANDS = [\"district\", \"DISTRICT\", \"ADM2_NAME\", \"NAME_2\", \"province\", \"PROVINCE\", \"ADM1_NAME\", \"NAME_1\", \"city\", \"CITY\"]\n",
        "admin_found = [c for c in ADMIN_CANDS if c in gdf.columns]\n",
        "\n",
        "kigali_districts = {\"gasabo\", \"kicukiro\", \"nyarugenge\"}\n",
        "\n",
        "if admin_found:\n",
        "    print(\"Admin columns found:\", admin_found)\n",
        "\n",
        "    district_cols = [c for c in admin_found if \"dist\" in c.lower() or \"adm2\" in c.lower() or \"name_2\" in c.lower()]\n",
        "    filtered = False\n",
        "\n",
        "    for dc in district_cols:\n",
        "        vals = set(clean_text(gdf[dc]).unique())\n",
        "        if vals & kigali_districts:\n",
        "            gdf = gdf[clean_text(gdf[dc]).isin(kigali_districts)].copy()\n",
        "            print(f\"Filtered to Kigali districts using {dc}. Rows now:\", len(gdf))\n",
        "            filtered = True\n",
        "            break\n",
        "\n",
        "    if not filtered:\n",
        "        for ac in admin_found:\n",
        "            vals = set(clean_text(gdf[ac]).unique())\n",
        "            if \"kigali\" in vals:\n",
        "                gdf = gdf[clean_text(gdf[ac]).eq(\"kigali\")].copy()\n",
        "                print(f\"Filtered to Kigali using {ac} == 'kigali'. Rows now:\", len(gdf))\n",
        "                break\n",
        "\n",
        "zone_uniques_after = clean_text(gdf[zone_col]).nunique()\n",
        "print(\"Unique zone values after filtering:\", zone_uniques_after)\n",
        "\n",
        "# Build zone_id\n",
        "gdf[\"zone_id\"] = clean_text(gdf[zone_col])\n",
        "\n",
        "if zone_uniques_after > 60:\n",
        "    district_like_cols = [c for c in gdf.columns if any(k in c.lower() for k in [\"district\", \"adm2\", \"name_2\"])]\n",
        "    if district_like_cols:\n",
        "        dc = district_like_cols[0]\n",
        "        gdf[\"zone_id\"] = clean_text(gdf[dc]) + \"__\" + clean_text(gdf[zone_col])\n",
        "        print(f\"Using composite zone_id = {dc} + {zone_col}\")\n",
        "    else:\n",
        "        print(\"No district-like column found to build composite key.\")\n",
        "\n",
        "print(\"Unique zone_id BEFORE dissolve:\", gdf[\"zone_id\"].nunique())\n",
        "\n",
        "# Dissolve\n",
        "g35 = gdf.dissolve(by=\"zone_id\", as_index=False)\n",
        "\n",
        "print(\"Unique zone_id AFTER dissolve:\", g35[\"zone_id\"].nunique())\n",
        "\n",
        "# Save\n",
        "g35 = g35[[\"zone_id\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
        "g35.to_file(\"kigali_35_sectors.geojson\", driver=\"GeoJSON\")\n",
        "\n",
        "print(\"Saved: kigali_35_sectors.geojson\")\n",
        "print(g35.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_5ccp4VzaxH",
        "outputId": "8029ee9d-667c-4cc2-e221-02db9c0397f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['GID_3', 'GID_0', 'COUNTRY', 'GID_1', 'NAME_1', 'NL_NAME_1', 'GID_2', 'NAME_2', 'NL_NAME_2', 'NAME_3', 'VARNAME_3', 'NL_NAME_3', 'TYPE_3', 'ENGTYPE_3', 'CC_3', 'HASC_3', 'geometry']\n",
            "CRS: EPSG:4326\n",
            "Rows: 422\n",
            "[candidate] NAME_3: unique=381\n",
            "Chosen zone_col: NAME_3 | uniques: 381\n",
            "Admin columns found: ['NAME_2', 'NAME_1']\n",
            "Filtered to Kigali districts using NAME_2. Rows now: 35\n",
            "Unique zone values after filtering: 35\n",
            "Unique zone_id BEFORE dissolve: 35\n",
            "Unique zone_id AFTER dissolve: 35\n",
            "Saved: kigali_35_sectors.geojson\n",
            "    zone_id                                           geometry\n",
            "0   bumbogo  POLYGON ((30.1401 -1.8636, 30.1399 -1.8322, 30...\n",
            "1   gahanga  POLYGON ((30.1038 -2.0599, 30.1037 -2.0566, 30...\n",
            "2   gatenga  POLYGON ((30.0785 -2.0269, 30.0783 -2.0318, 30...\n",
            "3   gatsata  POLYGON ((30.0384 -1.9419, 30.034 -1.9427, 30....\n",
            "4  gikomero  POLYGON ((30.229 -1.8977, 30.2188 -1.8964, 30....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import os, shutil\n",
        "\n",
        "#  Read the 35-sectors GeoJSON (the dissolved output)\n",
        "gdf = gpd.read_file(\"/content/kigali_35_sectors.geojson\")\n",
        "\n",
        "print(\"Zones:\", gdf[\"zone_id\"].nunique(), \" | Rows:\", len(gdf))  # should be 35\n",
        "\n",
        "# Write shapefile folder (clean folder first)\n",
        "out_dir = \"/content/kigali_35_sectors_shp\"\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "shp_path = os.path.join(out_dir, \"kigali_35_sectors.shp\")\n",
        "gdf.to_file(shp_path, driver=\"ESRI Shapefile\")\n",
        "\n",
        "#  Zip the shapefile components\n",
        "zip_path = shutil.make_archive(\"/content/kigali_35_sectors\", \"zip\", out_dir)\n",
        "print(\"Created ZIP:\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQB0uXq7eML",
        "outputId": "bbd19ddb-5363-4623-bfad-0fbdb1e56aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zones: 35  | Rows: 35\n",
            "Created ZIP: /content/kigali_35_sectors.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge NIDVI Dataset:** This stage prepares the satellite vegetation (NDVI) dataset for integration into the WI-DR zone-day forecasting dataset. Monthly NDVI values exported from Google Earth Engine for 2021–2023 are first loaded, cleaned, standardized, and merged into a single continuous monthly dataset covering all 35 Kigali sectors. Duplicate records are removed and data is sorted chronologically.\n",
        "\n",
        "Since the WI-DR forecasting pipeline operates at daily resolution, the monthly NDVI values are expanded into a complete sector-day time series. A full grid of all sector–date combinations is generated, and each day inherits its corresponding monthly NDVI value through forward-filling within each sector. This preserves temporal continuity while avoiding future data leakage. The result is a daily NDVI dataset aligned with the 35 Kigali sectors, ready to be merged with baseline waste, weather, and calendar-event datasets for forecasting and routing optimization."
      ],
      "metadata": {
        "id": "B1FhAGxNWA_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration: paths to monthly NDVI files exported from Google Earth Engine\n",
        "NDVI_FILES = {\n",
        "    2021: \"/content/drive/MyDrive/satellite_zone_monthly_ndvi_2021.csv\",\n",
        "    2022: \"/content/drive/MyDrive/satellite_zone_monthly_ndvi_2022.csv\",\n",
        "    2023: \"/content/drive/MyDrive/satellite_zone_monthly_ndvi_2023.csv\",\n",
        "}\n",
        "\n",
        "# Output files\n",
        "OUT_MONTHLY = \"/content/satellite_zone_monthly_ndvi_2021_2023.csv\"\n",
        "OUT_DAILY   = \"/content/satellite_zone_daily_2021_2023.csv\"\n",
        "\n",
        "# Date range for expanding to daily resolution\n",
        "START_DATE = \"2021-01-01\"\n",
        "END_DATE   = \"2023-12-31\"\n",
        "\n",
        "\n",
        "def load_ndvi_year(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load one yearly NDVI file and standardize column names and data types.\n",
        "    Ensures required columns exist and formats are correct.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Sometimes NDVI is exported as \"mean\" from Earth Engine\n",
        "    if \"ndvi\" not in df.columns and \"mean\" in df.columns:\n",
        "        df = df.rename(columns={\"mean\": \"ndvi\"})\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required = {\"zone_id\", \"date\", \"ndvi\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"{path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
        "\n",
        "    # Standardize formats\n",
        "    df[\"zone_id\"] = df[\"zone_id\"].astype(str).str.strip()\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df[\"ndvi\"] = pd.to_numeric(df[\"ndvi\"], errors=\"coerce\")\n",
        "\n",
        "    # Remove rows missing key identifiers\n",
        "    df = df.dropna(subset=[\"zone_id\", \"date\"])\n",
        "\n",
        "    return df[[\"zone_id\", \"date\", \"ndvi\"]]\n",
        "\n",
        "\n",
        "def merge_ndvi_years(files_dict: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge all yearly NDVI datasets into a single continuous monthly dataset.\n",
        "    Removes duplicates and sorts chronologically.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    for year, path in sorted(files_dict.items()):\n",
        "        df = load_ndvi_year(path)\n",
        "        df[\"year_source\"] = year\n",
        "        frames.append(df)\n",
        "\n",
        "    out = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Remove duplicate zone_id + date entries if any\n",
        "    out = out.drop_duplicates(subset=[\"zone_id\", \"date\"], keep=\"last\")\n",
        "\n",
        "    # Sort by sector and time\n",
        "    out = out.sort_values([\"zone_id\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def monthly_to_daily_forwardfill(ndvi_monthly: pd.DataFrame,\n",
        "                                 start_date: str,\n",
        "                                 end_date: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert monthly NDVI into daily NDVI by forward-filling values\n",
        "    within each zone (no future information leakage).\n",
        "    \"\"\"\n",
        "    df = ndvi_monthly.copy()\n",
        "\n",
        "    # Convert each record to its corresponding month\n",
        "    df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "    zones = df[\"zone_id\"].unique()\n",
        "    daily_dates = pd.date_range(start_date, end_date, freq=\"D\")\n",
        "\n",
        "    # Create full Zone × Day grid\n",
        "    grid = pd.MultiIndex.from_product(\n",
        "        [zones, daily_dates],\n",
        "        names=[\"zone_id\", \"date\"]\n",
        "    ).to_frame(index=False)\n",
        "\n",
        "    grid[\"month\"] = grid[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "    # Merge monthly NDVI into daily grid\n",
        "    daily = grid.merge(df[[\"zone_id\", \"month\", \"ndvi\"]],\n",
        "                       on=[\"zone_id\", \"month\"],\n",
        "                       how=\"left\")\n",
        "\n",
        "    daily = daily.sort_values([\"zone_id\", \"date\"])\n",
        "\n",
        "    # Forward-fill NDVI per zone (no future-to-past filling)\n",
        "    daily[\"ndvi\"] = daily.groupby(\"zone_id\")[\"ndvi\"].ffill()\n",
        "\n",
        "    # Clean output\n",
        "    daily = daily.drop(columns=[\"month\"])\n",
        "    daily[\"date\"] = daily[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    return daily\n",
        "\n",
        "\n",
        "# Merge monthly NDVI from all years\n",
        "ndvi_monthly = merge_ndvi_years(NDVI_FILES)\n",
        "ndvi_monthly.to_csv(OUT_MONTHLY, index=False)\n",
        "print(\"Saved merged monthly NDVI:\", OUT_MONTHLY, ndvi_monthly.shape)\n",
        "\n",
        "# Expand monthly NDVI → daily NDVI\n",
        "ndvi_daily = monthly_to_daily_forwardfill(ndvi_monthly, START_DATE, END_DATE)\n",
        "ndvi_daily.to_csv(OUT_DAILY, index=False)\n",
        "print(\"Saved daily NDVI:\", OUT_DAILY, ndvi_daily.shape)\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Unique zones:\", ndvi_monthly[\"zone_id\"].nunique())\n",
        "print(\"Monthly rows (expected 35*36=1260):\", len(ndvi_monthly))\n",
        "print(\"Daily rows (expected 35*1095=38325):\", len(ndvi_daily))\n",
        "print(ndvi_monthly.head())\n",
        "print(ndvi_daily.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gYIhlOR0rbM",
        "outputId": "9b871775-2a23-4261-bd8e-422862347830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved merged monthly NDVI: /content/satellite_zone_monthly_ndvi_2021_2023.csv (1260, 4)\n",
            "Saved daily NDVI: /content/satellite_zone_daily_2021_2023.csv (38325, 3)\n",
            "Unique zones: 35\n",
            "Monthly rows (expected 35*36=1260): 1260\n",
            "Daily rows (expected 35*1095=38325): 38325\n",
            "   zone_id       date      ndvi  year_source\n",
            "0  bumbogo 2021-01-01  0.534608         2021\n",
            "1  bumbogo 2021-02-01  0.307311         2021\n",
            "2  bumbogo 2021-03-01  0.468052         2021\n",
            "3  bumbogo 2021-04-01  0.489327         2021\n",
            "4  bumbogo 2021-05-01  0.454134         2021\n",
            "   zone_id        date      ndvi\n",
            "0  bumbogo  2021-01-01  0.534608\n",
            "1  bumbogo  2021-01-02  0.534608\n",
            "2  bumbogo  2021-01-03  0.534608\n",
            "3  bumbogo  2021-01-04  0.534608\n",
            "4  bumbogo  2021-01-05  0.534608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge all Integrated Dataset**"
      ],
      "metadata": {
        "id": "DY0MgH55XH3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "baseline = pd.read_csv(\"/content/drive/MyDrive/WI-DR/historical_waste_kigali_sectors.csv\")\n",
        "events   = pd.read_csv(\"/content/drive/MyDrive/WI-DR/events_calendar_zone_daily_2021_2023.csv\")\n",
        "weather  = pd.read_csv(\"/content/drive/MyDrive/WI-DR/weather_daily_kigali.csv\")\n",
        "ndvi     = pd.read_csv(\"/content/drive/MyDrive/WI-DR/satellite_zone_daily_2021_2023.csv\")\n",
        "\n",
        "def clean_text(s):\n",
        "    return (s.astype(str)\n",
        "             .str.strip()\n",
        "             .str.lower()\n",
        "             .str.replace(r\"\\s+\", \" \", regex=True))\n",
        "\n",
        "# Standardize dates + ids\n",
        "for df in [baseline, events, weather, ndvi]:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    if \"zone_id\" in df.columns:\n",
        "        df[\"zone_id\"] = df[\"zone_id\"].astype(str).str.strip()\n",
        "\n",
        "# Weather is city-level → replicate across all zones\n",
        "zones = baseline[[\"zone_id\"]].drop_duplicates()\n",
        "weather_z = zones.merge(weather, how=\"cross\")\n",
        "\n",
        "# Merge baseline + events + weather first\n",
        "integrated = (baseline\n",
        "              .merge(events,    on=[\"zone_id\",\"date\"], how=\"left\")\n",
        "              .merge(weather_z, on=[\"zone_id\",\"date\"], how=\"left\"))\n",
        "\n",
        "\n",
        "# create a shared cleaned key and merge on (zone_key + date)\n",
        "if \"zone_name\" not in integrated.columns:\n",
        "    raise ValueError(\"baseline must contain a 'zone_name' column to merge NDVI correctly.\")\n",
        "\n",
        "integrated[\"zone_key\"] = clean_text(integrated[\"zone_name\"])\n",
        "ndvi[\"zone_key\"] = clean_text(ndvi[\"zone_id\"])\n",
        "\n",
        "# Merge NDVI using zone_key + date\n",
        "integrated = integrated.merge(\n",
        "    ndvi[[\"zone_key\", \"date\", \"ndvi\"]],\n",
        "    on=[\"zone_key\", \"date\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop helper column\n",
        "integrated = integrated.drop(columns=[\"zone_key\"])\n",
        "\n",
        "# Save integrated dataset\n",
        "OUT_FILE = \"/content/drive/MyDrive/WI-DR/widr_zone_day_integrated_2021_2023.csv\"\n",
        "integrated.to_csv(OUT_FILE, index=False)\n",
        "\n",
        "print(\"Integrated dataset saved:\", OUT_FILE)\n",
        "print(\"Shape:\", integrated.shape)\n",
        "print(\"NDVI missing %:\", integrated[\"ndvi\"].isna().mean() * 100)\n",
        "print(integrated[[\"date\",\"zone_id\",\"zone_name\",\"ndvi\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZCj8YCeXGGc",
        "outputId": "63808fe4-59ca-4e67-ee3a-f946bc4109d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrated dataset saved: /content/drive/MyDrive/WI-DR/widr_zone_day_integrated_2021_2023.csv\n",
            "Shape: (38325, 14)\n",
            "NDVI missing %: 0.0\n",
            "        date zone_id   zone_name      ndvi\n",
            "0 2021-01-01  SEC_01      Gitega  0.245045\n",
            "1 2021-01-01  SEC_02    Kanyinya  0.599476\n",
            "2 2021-01-01  SEC_03      Kigali  0.494406\n",
            "3 2021-01-01  SEC_04  Kimisagara  0.294643\n",
            "4 2021-01-01  SEC_05  Mageregere  0.468142\n"
          ]
        }
      ]
    }
  ]
}